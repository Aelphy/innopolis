\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{ {images/} }

\textwidth=431pt
\textheight=600pt
\hoffset=-30pt
\voffset=-30pt

\usepackage{graphicx}
\usepackage{amsmath}
\makeatletter
\renewcommand{\@oddhead}{%
\vbox{%
\hbox to \textwidth{\strut \textit{Decision Making, Final exam, Usvyatsov Mikhail} \hfill }
%\hbox to\textwidth{Лист\hfill Страница~\arabic{page}~из 2}
\hrule
\vspace{12pt}
}}
\renewcommand{\@oddfoot}{}
\makeatother

\begin{document}

%\tableofcontents

%\newpage

\begin{center}
\textbf{Take home exam;\\
Due: January 7}
\end{center}

\bigskip
	
\textbf{Exercise 1}		
\newcounter{bcounter}
\begin{list}{(\alph{bcounter})~}{\usecounter{bcounter}}
\item 
We know that:
$\int_{-\infty}^{\infty} PDF(x) dx = 1$, as far that we know that the 0 < x < 4, we can change integration limits, so $\int_{0}^{4} \dfrac{c}{\sqrt{x}} dx = 1$.
Finally we can conclude that: \\
 с = $\dfrac{1}{4}$
\item
$CDF(x) = \int_{-\infty}^{x} PDF(x) dx$\\
CDF(x) = 
$\begin{cases}
0, x < 0 \\
\dfrac{\sqrt{x}}{2}, 0 \leqslant x \leqslant 4 \\
 1, x > 4
\end{cases}$
\item
P(x < 0.25) = $CDF(0.25) = 0.25$\\
P(x > 1) = $1 - CDF(1) = 0.5$\\
\item
$Y = \sqrt{X}$\\
$F_Y(y) = P(Y < y) = P(\sqrt{X} < y) = P(X < y^2) = CDF(y^2)$\\
$F_Y(y) = 
\begin{cases}
0, y < 0 \\
\dfrac{y}{2}, 0 \leqslant y \leqslant 2 \\
 1, y > 2
\end{cases}$
\item
$E(y) = \int_{-\infty}^{\infty} y \cdot PDF(y) dy$\\
$E(y) = \int_{0}^{2} \dfrac{y}{2} dy = 1$\\
$Var(y) = E(y^2) - E^2(y) = \int_{0}^{2} \dfrac{y^2}{2} dy - 1 = \dfrac{1}{3}$
\end{list}
\medskip

\textbf{Exercise 2}

\newcounter{ccounter}
\begin{list}{(\alph{ccounter})~}{\usecounter{ccounter}}
\item
We know from the CLT that, $\sqrt{n}\left(\dfrac{1}{n} \sum^{n}_{i = 1} X_i - Ex\right) \sim N(0,\sigma^2)$, so we can derive, that: $\overline{X} \sim N\left(Ex, \dfrac{\sigma^2}{n}\right)$\\

From the initial distribution we can find Ex and $\sigma^2$:

PDF(x) = $\dfrac{d F(x)}{dx} = 4 x^{-5}$\\
$E(x) = \int_{-\infty}^{\infty} x \cdot PDF(x) dx = \int_{1}^{\infty} \dfrac{4}{x^{-4}} dx = \dfrac{4}{3}$ \\
$\sigma^2 = Ex^2 - E^2x = \int_{1}^{\infty} \dfrac{4}{x^{-3}} dx = \dfrac{2}{9}$, and so:\\
$\overline{X} \sim N\left(\dfrac{4}{3}, \dfrac{2}{9 \cdot n}\right)$\\
\item
Let g(x) = ln(x)\\

We can use delta method, because $g'(Ex) \neq 0$ and g(x) has a derivative equals to $\dfrac{1}{x}$

As far as we know that:\\
$\sqrt{n}\left(\overline{X} - \dfrac{4}{3}\right) \sim N(0,\sigma^2)$\\
We can conclude from delta method:\\
$\sqrt{n} \left(g(\overline{X}) - g\left(\dfrac{4}{3}\right)\right) \sim N\left(0, \sigma^2 \left[ g'\left(\dfrac{4}{3}\right) \right]^2\right)$, and so:\\
$ln\left(x\right) \sim N\left(ln\left(\dfrac{4}{3}\right), \dfrac{1}{8 \cdot n}\right)$
\item
We know that: \\
$\dfrac{3 \cdot \sqrt{n}}{\sqrt{2}} \left(\overline{X} - \dfrac{4}{3}\right) \sim N(0,1)$\\
Thus, $\dfrac{9 \cdot n}{2}\left(\overline{X} - \dfrac{4}{3}\right)^2 \sim \chi_1^2$\\
$n \cdot \left(\overline{X} - \dfrac{4}{3}\right)^2 \sim \dfrac{2}{9} \cdot \chi_1^2$\\
$n \cdot \left(\overline{X} - \dfrac{4}{3}\right)^2 = n \cdot \left(\overline{X} - 0.8 + 0.8 - \dfrac{4}{3}\right)^2 = n \cdot \left(\overline{X} - 0.8\right)^2 - 2 \cdot \dfrac{8 \cdot n}{15} \cdot \left(\overline{X} - 0.8\right) + \dfrac{64}{15^2}$\\
$2 \cdot \dfrac{8 \cdot n}{15} \cdot \left(\overline{X} - 0.8\right) \sim N\left(\dfrac{32}{15}, \dfrac{15^2}{16^2 \cdot n^2}\right)$\\
Let us define $Z = n \cdot \left(\overline{X} - \dfrac{4}{3}\right)^2 + \dfrac{16 \cdot n}{15} \cdot \left(\overline{X} - 0.8\right) - \dfrac{64}{15^2}$\\
As far as Z consists of two random variables, the pdf of Z is difficult to find.\\
We can just write that g(z), where g(z) is the PDF of Z is equal to\\ g(z) = $\int^\infty_{-\infty} f_1(x) \cdot f_2(z - x) dx$, where $f_1(x)$ is the PDF of $n \cdot \left(\overline{X} - \dfrac{4}{3}\right)^2$ and $f_2(x)$ is the PDF of $\dfrac{16 \cdot n}{15} \cdot \left(\overline{X} - 0.8\right)$\\
However we can see, that $\sigma^2$ of $\dfrac{16 \cdot n}{15} \cdot \left(\overline{X} - 0.8\right)$ asymptotically goes to zero, and so, we can think that it is the constant.\\
So, we can think, that asymptotically $n \cdot \left(\overline{X} - 0.8\right)^2 \sim \dfrac{2}{9} \cdot \chi_1^2 + 1.848$
\end{list}

\textbf{Exercise 3}

\begin{list}{(\alph{ccounter})~}{\usecounter{ccounter}}
\item
\begin{enumerate}
\item
From Chebyshev-Markov inequality we know that:\\
$P\left( \vert \overline{X} - \mu \vert > \varepsilon \right) \leq \dfrac{\sigma^2}{n \cdot \varepsilon^2}$\\
We can find that $\varepsilon = 1.4$ and so, the probability that $\overline{X}$ is not in interval less than 25 \%
\item
From CLT we know that $\overline{X} \sim N\left( \mu, \dfrac{\sigma^2}{n} \right)$\\
To find the probability that $\overline{X}$ is not in the interval we have to find the value of Laplass function.\\
$P\left( \vert N(128, \dfrac{6.3^2}{81}) - \mu   \vert > \varepsilon \right) = 2 \cdot \Phi\left(\dfrac{\mu + \varepsilon}{\sigma^2}\right)$ = 0.02394
\end{enumerate}
\item
$129 \pm 1.96 \cdot 6.3 = [116.652, 141.348] $
\end{list}
\medskip

\medskip		

\textbf{Exercise 4}

$F(x) = 1 - \dfrac{Q}{x}$, x $\geq Q$\\
$f(x \vert Q) = \dfrac{d F(x)}{dx} = \dfrac{Q}{x^2}$ ,  x $\geq Q$\\
Let us define $X_{(1)} = min_{1 \leq i \leq n}X_i$\\
Joint pmf of $X_1..X_n$ is :\\
$f(x \vert Q) = \prod^n_{i = 1} \dfrac{Q}{X^2_i} I_{(Q, \infty)})(X_i) = I_{(Q, \infty)})(X_{(1)}) \prod^n_{i = 1} \dfrac{Q}{X^2_i} = I_{(Q, \infty)})(X_{(1)}) \dfrac{Q^n}{X^2_1 \cdot X^2_2 \cdot ... \cdot X^2_n }$\\

By the Factorization Theorem we can show defining:\\
$h(x) = \dfrac{1}{X^2_1 \cdot X^2_2 \cdot ... \cdot X^2_n }$\\
$g(t | Q) = Q^n  I_{(Q, \infty)})(t)$\\
that  $f(x \vert Q) = h(x) \cdot g(T(x) | Q)$.\\
And so, T(x) is sufficient, QED
\medskip

\textbf{Exercise 5}

\begin{list}{(\alph{ccounter})~}{\usecounter{ccounter}}
\item Let $T = X_1 + X_2 + \cdots + X_n$ and let $f_t\left(x_1, x_2, \cdots, x_n | \theta\right)$ be the joint density of $X_1, X_2, \cdots, X_n$.


\[f_t\left(x_1, x_2, \cdots, x_n | \theta\right) =\left\{
\begin{array}{l}
{\displaystyle \prod_{i=1}^{n}} \theta e^{-\theta x}, if\ x > 0,\\
0, otherwise
\end{array} \right. = \left\{
\begin{array}{l}
\theta^n e^{-\theta \sum_{i=1}^n x_i}, if\ x > 0,\\
0, otherwise
\end{array} \right. = \] 

\[= \theta^n e^{-\theta t}\cdot h\left(x_1, x_2, \cdots, x_n\right)=g\left(t, \theta\right)h\left(x_1, x_2, \cdots, x_n\right) = \theta^n e^{-\theta t}\]

Where 

\[g\left(t, \theta\right) = \theta^{n}e^{-\theta t}\]

\[h\left(x_1, \cdots, x_n\right) = \left\{
\begin{array}{l}
1, if\ x > 0,\\
0, otherwise
\end{array} \right.\]

Hence, $T$ is a sufficient statistic for $\theta$.
\item
\[f\left(x; \theta\right)=\theta e^{-\theta x}\]

\[l\left(x; \theta\right)=log\left(\theta e^{-\theta x}\right)\]

Fisher information is:

\[I\left(\theta\right)=-E\left(\dfrac{\eth^2 l\left(x; \theta\right)}{\eth \theta^2}\right) = -E\left(\-\dfrac{1}{\theta^2}\right)=\dfrac{1}{\theta^2}\]

Hence, Cramer-Rao lower bound:

\[Var\left(\hat{\theta}\right)=\theta^2\]
\item The likelihood function for $\theta$, given an iid sample $X=\left(x_1, x_2, \cdots, x_n\right)$:

\[L\left(\theta\right) = {\displaystyle \prod_{i=1}^{n}} \theta e^{-\theta x} = \theta^n e^{-\theta n \overline{x}}, where\ \overline{x} = \dfrac{1}{n}\sum_{i=1}^n x_i\]

The derivative if the likelihood function's logarithm is

\[\dfrac{\eth}{\eth \theta}ln\left(L\left(\theta\right)\right)=\dfrac{\eth}{\eth \theta}\left(n ln\theta-\theta n \overline{x}\right)=\left\{
\begin{array}{l}
>0, 0 < \theta < \dfrac{1}{\overline{x}},\\
=0, \theta=\dfrac{1}{\overline{x}},\\
<0, \theta > \dfrac{1}{\overline{x}}
\end{array} \right. \]

Consequently the MLE

\[\hat{\theta}=\dfrac{1}{\overline{x}}\]

The asymptotic distribution of the estimator is $N\left(\theta, \theta^2\right)$.

\item
\[L\left(\theta\right)=\theta^n e^{-\theta n \overline{x}}\]
\[H_0: \theta = 1\]
\[H_1: \theta>1, \theta = \theta_a\]

Ratio of the likelihood functions is:

\[\dfrac{L\left(1\right)}{L\left(\theta_a\right)}=\dfrac{e^{-n\overline{x}}}{\theta_a^n e^{-\theta_a n \overline{x}}}\leq k\]

\[e^{n\overline{x}\left(\theta_a-1\right)}\theta_a^{-n}\leq k\]

\[\left(\theta_a-1\right)n\overline{x}-n ln\left(\theta_a\right)\leq ln(k)\]
\[\left(\theta_a-1\right)\sum x_i \leq ln\left(k\right)+nln\left(\theta_a\right)\]

\[\sum x_i \leq \dfrac{ln\left(k\right)+nln\left(\theta_a\right)}{\left(\theta_a-1\right)}=k^*\]

Therefore,  the best critical region of size $\alpha$ for test $H_0: \theta=1$ against each simple alternative $H_1: \theta=\theta_a$, where $\theta_a>1$ if given by:

\[ C =\left\{\left(x_1, \cdots, x_n\right), \sum_{i=1}^n x_i\leq k^*\right\}\]

Where $k^*$ is selected 

\[\alpha = P\left(\sum_{i=1}^n x_i \leq k^*, when\  \theta=1\right)\]

\[0.05 = P\left(\sum_{i=1}^n x_i \leq k^*, when\  \theta=1\right)\]

\[\sum_{i=1}^n x_i = \eta \thicksim \Gamma\left(100, 1\right)\]

\[F_\eta\left(k^*\right)=P\left(\eta\leq k^*\right)=0.05\]

\[\int_0^{k^*}f_\eta\left( x\right)dx=\int_0^{k^*}x^0 \dfrac{e^{-\dfrac{x}{100}}}{100\Gamma\left(1\right)}dx=0.05\]

\[\int_0^{k^*}e^{-\dfrac{x}{100}}dx=5,\ t=-\dfrac{x}{100},\ dt=-\dfrac{dx}{100},\ dx=-100dt, x=k^* \rightarrow t=-\dfrac{k^*}{100} \]

\[\int _0^{-\dfrac{k^*}{100}}e^t dt=5\]

\[\int _0^{-\dfrac{k^*}{100}}e^t dt =-0.05\]

\[e^{-\dfrac{k^*}{100}}-1=-0.05\]

\[k^*=-100ln\left(0.95\right)\]

Hence, our test is

\[\sum_{i=1}^n x_i \leq -100ln\left(0.95\right)\]

\item
The Wald Statistic is:

\[Z_w=\dfrac{\left(\dfrac{1}{\overline{x}}-1\right)^2}{\theta} = \left({\overline{x}}-1\right)^2\]

In our task 

\[Z_w\leq \chi^2\left(99\right)=81.4\]

Hence

\[\left({\overline{x}}-1\right)^2\leq 81.4\ if\ H_0, H_1\ otherwise\]

\end{list}

\medskip

\textbf{Exercise 6}

Note that:\\

$L(\theta) \propto \theta^{\sum^n_{i = 1} x_i} \cdot \left( 1 - \theta \right)^{1 - \sum^n_{i = 1} x_i}$, so we can find:\\
$\dfrac{L(\theta_0)}{L(\theta_1)} = \left( \dfrac{\theta_0}{\theta_1} \right)^{\sum^n_{i = 1} x_i} \cdot \left( \dfrac{1- \theta_0}{ 1 - \theta_1} \right)^{1 - \sum^n_{i = 1} x_i}$ \\
Since this is decreasing function of $\sum^n_{i = 1} x_i$, we accept $H_1$ if $\sum^n_{i = 1} x_i > c$\\
We have that $\sum^n_{i = 1} x_i \sim \mathrm{Bin}(n, \theta)$ and from CLT $\mathrm{Bin}(n,\theta) \approx N( n\theta, n\theta (1 - \theta) )$, and hence  $\dfrac{\sum^n_{i = 1} x_i - n\theta}{\sqrt{n \theta (1 - \theta)}} \sim N(0,1)$\\
We now wish to find n so that:\\
$P_{\theta = 0.5} \left( \sum^n_{i = 1} x_i < c \right) = 0.9$\\
$P_{\theta = 0.6} \left( \sum^n_{i = 1} x_i < c \right) = 0.1$

This amounts to solving the equations\\
$1.28 = \dfrac{c - 0.5n}{\sqrt{0.25n}}$\\
$-1.28 = \dfrac{c - 0.6n}{\sqrt{0.24n}}$ and so we got, that n > 160.54, it means, that n = 161
\end{document}